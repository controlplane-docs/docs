---
title: Kubernetes Operator
description: Deploy and manage Control Plane resources using Kubernetes custom resource definitions (CRDs).
---

The Control Plane Kubernetes Operator enables you to manage Control Plane resources directly from your Kubernetes cluster using custom resource definitions (CRDs). It bridges the gap between Kubernetes-native workflows and Control Plane infrastructure.

## What you'll achieve

By the end of this guide, you will have:

1. A Kubernetes cluster with the Control Plane operator installed
2. Authentication configured between your cluster and Control Plane
3. The ability to deploy and manage Control Plane resources using Kubernetes CRDs
4. Optional ArgoCD integration for GitOps workflows

## When to use this

<CardGroup cols={2}>
  <Card title="GitOps workflows" icon="code-branch">
    Manage Control Plane resources alongside your Kubernetes manifests in Git
  </Card>
  <Card title="ArgoCD integration" icon="arrows-rotate">
    Deploy Control Plane resources through ArgoCD applications
  </Card>
  <Card title="Kubernetes-native experience" icon="dharmachakra">
    Manage GVCs, workloads, secrets, and more using CRDs
  </Card>
  <Card title="Infrastructure as Code" icon="file-code">
    Define your entire Control Plane infrastructure declaratively
  </Card>
</CardGroup>

## Supported resources

The operator manages the following Control Plane resource types through CRDs:

| Resource | Description |
|----------|-------------|
| `agent` | Secure connectivity to private networks and cloud provider services |
| `auditctx` | Tamper-proof audit trail for tracking actions |
| `cloudaccount` | Cloud provider integrations (AWS, GCP, Azure, NGS) |
| `domain` | Custom domain mapping with TLS and geo-routing |
| `group` | Membership collection of users and service accounts |
| `gvc` | Global Virtual Cloud - groups workloads and defines deployment locations |
| `identity` | Grants workloads access to cloud resources and private networks |
| `ipset` | Reserved public IP addresses for workloads |
| `location` | Cloud regions where workloads can be deployed |
| `mk8s` | Managed Kubernetes clusters across cloud providers |
| `org` | Top-level context for all Control Plane resources |
| `policy` | Grants permissions to principals on target resources |
| `secret` | Encrypted storage for credentials and sensitive data |
| `serviceaccount` | Machine identity for headless API operations |
| `volumeset` | Persistent storage with snapshots and auto-scaling |
| `workload` | Application containers running on Control Plane |

## Prerequisites

<AccordionGroup>
<Accordion title="Kubernetes cluster">
  A running Kubernetes cluster (v1.19+). This can be:
  - A managed cluster (EKS, GKE, AKS)
  - A local cluster (kind, minikube, Docker Desktop)
  - Any conformant Kubernetes distribution

  If you don't have a cluster, see [Quick start (local cluster)](#quick-start-local-cluster) to set one up.
</Accordion>

<Accordion title="Helm CLI">
  Install Helm v3.0+ for deploying the operator:

  ```bash
  # macOS
  brew install helm

  # Linux
  curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

  # Windows
  choco install kubernetes-helm
  ```

  For other installation methods, see the [Helm installation guide](https://helm.sh/docs/intro/install/).
</Accordion>

<Accordion title="kubectl configured">
  Ensure kubectl is configured and can communicate with your cluster:

  ```bash
  kubectl cluster-info
  ```
</Accordion>

<Accordion title="Control Plane account">
  You need to be signed up to Control Plane and have access to an org. If you don't already:

  - [Sign up](https://console.cpln.io/signup) for Control Plane and create a [billing account](/concepts/billing)
  - [Create an Org](/guides/create-org)

  You also need permissions to create service accounts within your org.
</Accordion>
</AccordionGroup>

## Quick start (local cluster)

If you don't have a Kubernetes cluster, you can set up a local one for testing.

<Note>
Docker must be installed and running on your machine for local Kubernetes clusters.
</Note>

<Tabs>
  <Tab title="Repo Quick Start">
    Clone the operator repository and run the quick start command to create a local kind cluster with the operator pre-installed:

    ```bash
    git clone https://github.com/controlplane-com/k8s-operator.git
    cd k8s-operator
    make cluster-quickstart
    ```

    This creates a kind cluster and installs cert-manager and the operator automatically.

    Verify the cluster is running:

    ```bash
    kubectl cluster-info
    ```
  </Tab>
  <Tab title="kind">
    [Install kind](https://kind.sigs.k8s.io/docs/user/quick-start/#installation), then create a cluster:

    ```bash
    kind create cluster --name cpln-operator

    # Verify
    kubectl cluster-info
    ```

    Then continue with the [installation steps](#installation) below.
  </Tab>
  <Tab title="minikube">
    [Install minikube](https://minikube.sigs.k8s.io/docs/start/), then start a cluster:

    ```bash
    minikube start

    # Verify
    kubectl cluster-info
    ```

    Then continue with the [installation steps](#installation) below.
  </Tab>
</Tabs>

## Installation

<Info>
If you used the **Repo Quick Start** above, skip to [Step 3: Configure authentication](#using-the-cli-recommended) â€” cert-manager and the operator are already installed.
</Info>

### Step 1: Install cert-manager

The operator requires [cert-manager](https://cert-manager.io/) for webhook certificate management.

```bash
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.16.3/cert-manager.yaml
```

Wait for cert-manager to be ready:

```bash
kubectl wait --for=condition=Available deployment --all -n cert-manager --timeout=300s
```

### Step 2: Install the operator

Add the Control Plane Helm repository and install the operator:

```bash
# Add the Helm repository
helm repo add cpln https://controlplane-com.github.io/k8s-operator
helm repo update

# Install the operator in the controlplane namespace
helm install cpln-operator cpln/cpln-operator \
  -n controlplane \
  --create-namespace
```

Verify the operator is running:

```bash
kubectl get pods -n controlplane -l app=operator
```

### Step 3: Configure authentication

The operator needs credentials to communicate with the Control Plane API. You have two options:

<Tabs>
  <Tab title="Using the CLI (Recommended)">
    The CLI automatically creates the service account and Kubernetes secret:

    ```bash
    cpln operator install \
      --serviceaccount k8s-operator \
      --org YOUR_ORG_NAME
    ```

    This command:

    <Steps>
      <Step title="Creates a service account">
        Creates a service account named `k8s-operator` if it doesn't already exist
      </Step>
      <Step title="Assigns group membership">
        Adds the service account to the `superusers` group (customizable with `--serviceaccount-group`)
      </Step>
      <Step title="Generates authentication key">
        Creates a new key for the service account
      </Step>
      <Step title="Creates Kubernetes secret">
        Stores the credentials as a secret in the `controlplane` namespace
      </Step>
    </Steps>

    **Command options:**

    | Option | Description |
    |--------|-------------|
    | `--serviceaccount, -s` | Service account name (required) |
    | `--serviceaccount-group, -g` | Group to assign (default: `superusers`) |
    | `--export` | Export YAML to stdout instead of applying to cluster |
    | `--org` | Target organization |

    <Tip>
    Use `--export` to review or store the Kubernetes resources before applying:

    ```bash
    # Export to a file for review or GitOps
    cpln operator install \
      --serviceaccount k8s-operator \
      --org YOUR_ORG_NAME \
      --export > operator-secret.yaml

    # Apply later
    kubectl apply -f operator-secret.yaml
    ```
    </Tip>

    <Tip>
    For production, assign the service account to a group with limited permissions:

    ```bash
    cpln operator install \
      --serviceaccount k8s-operator \
      --serviceaccount-group operators \
      --org YOUR_ORG_NAME
    ```
    </Tip>
  </Tab>
  <Tab title="Manual setup">
    If you prefer manual control, create the secret yourself:

    **1. Create a service account in Control Plane:**

    ```bash
    cpln serviceaccount create --name k8s-operator --org YOUR_ORG_NAME
    ```

    **2. Add it to a group with appropriate permissions:**

    ```bash
    cpln group add-member superusers --serviceaccount k8s-operator --org YOUR_ORG_NAME
    ```

    **3. Generate a key:**

    ```bash
    cpln serviceaccount add-key k8s-operator --description "cpln operator" --org YOUR_ORG_NAME -o json
    ```

    Save the `key` value from the output.

    **4. Create the Kubernetes secret:**

    ```bash
    # Create the namespace if it doesn't exist
    kubectl create namespace controlplane --dry-run=client -o yaml | kubectl apply -f -
    ```

    Create a file named `operator-secret.yaml` with the following content:

    ```yaml
    apiVersion: v1
    kind: Secret
    metadata:
      name: YOUR_ORG_NAME
      namespace: controlplane
      labels:
        app.kubernetes.io/managed-by: cpln-operator
      annotations:
        cpln.io/service-account: k8s-operator
    data:
      token: BASE64_ENCODED_KEY  # base64 encode your key: echo -n "YOUR_KEY" | base64
    ```

    Apply the secret:

    ```bash
    kubectl apply -f operator-secret.yaml
    ```
  </Tab>
</Tabs>

## Generating CRD manifests

You don't need to write CRD manifests from scratch. Control Plane can export any resource in Kubernetes CRD format.

<Tabs>
  <Tab title="Console UI">
    **Export existing resources:**

    1. Navigate to any resource (GVC, workload, identity, etc.)
    2. Click the **Export** dropdown in the upper right corner
    3. Select **K8s CRD** to download the manifest

    **Preview during creation:**

    1. Create a new resource and configure it using the UI
    2. Before clicking Create, click **Preview**
    3. Select **K8s CRD** to generate the manifest
    4. Copy or download the YAML without deploying

    This lets you use the UI's guided configuration, then store the manifest in Git for GitOps deployments.
  </Tab>
  <Tab title="CLI">
    Export any existing resource with the `-o crd` flag:

    ```bash
    # Export a workload
    cpln workload get my-workload --gvc production -o crd --org my-org

    # Export a GVC
    cpln gvc get production -o crd --org my-org

    # Export an identity
    cpln identity get my-identity --gvc production -o crd --org my-org

    # Save directly to a file
    cpln workload get my-workload --gvc production -o crd --org my-org > workload.yaml
    ```
  </Tab>
</Tabs>

<Tip>
Combine this with ArgoCD: configure resources in the console, export as CRDs, commit to your Git repository, and let ArgoCD handle deployments automatically.
</Tip>

## Deploying resources

All Control Plane resources are managed through CRDs. Each resource requires:
- `org`: The target Control Plane organization
- `gvc`: The target GVC (for GVC-scoped resources like workloads and identities)

<Note>
We recommend organizing resources by namespace:
- One namespace per GVC for GVC-scoped resources (workloads, identities, volumesets)
- One namespace per org for org-scoped resources (GVCs, secrets, policies)
</Note>

<Warning>
The CRD structure differs from standard Kubernetes resources. Fields like `org`, `gvc`, and `description` are at the top level, not inside `spec`. Always use the [export feature](#generating-crd-manifests) to generate accurate manifests.
</Warning>

### Apply with kubectl

You can apply CRD manifests directly using kubectl. Save the following GVC manifest to a file (e.g., `gvc.yaml`):

```yaml
apiVersion: cpln.io/v1
kind: gvc
metadata:
  name: my-gvc
  namespace: default
org: YOUR_ORG_NAME  # Replace with your Control Plane org name
description: my-gvc
spec:
  staticPlacement:
    locationLinks:
      - //location/aws-eu-central-1
```

Apply it to your cluster:

```bash
kubectl apply -f gvc.yaml
```

Verify the resource was created:

```bash
kubectl get gvcs
```

The operator syncs the CRD to Control Plane. You can verify the GVC exists:

```bash
cpln gvc get my-gvc --org YOUR_ORG_NAME
```

### GitOps with ArgoCD

For production workflows, store your CRD manifests in a Git repository or package them as a Helm chart, and use [ArgoCD](#argocd-integration) to manage deployments automatically.

## Preventing resource deletion

Deleting a Kubernetes resource while the operator is installed and running will remove the corresponding resource from Control Plane. You can prevent this by adding the `cpln.io/resource-policy: keep` annotation to the metadata:

```yaml
kind: gvc
apiVersion: cpln.io/v1
org: your-org-name
metadata:
  name: production
  annotations:
    cpln.io/resource-policy: keep
spec:
  # ...
```

With this annotation, deleting the Kubernetes resource will not delete the Control Plane resource.

## ArgoCD integration

The operator integrates seamlessly with ArgoCD for GitOps workflows. Once the operator is installed, you can point ArgoCD at a Git repository containing YAML manifests or a Helm chart.

<Note>
This section assumes ArgoCD is already installed on your cluster. See the [ArgoCD installation guide](https://argo-cd.readthedocs.io/en/stable/getting_started/) if you haven't set it up yet.
</Note>

### Defining ArgoCD applications

An ArgoCD Application defines what to deploy (source) and where to deploy it (destination). For Control Plane CRDs, you can use either a Helm chart or raw YAML manifests stored in a Git repository.

Save the manifest to a file (e.g., `app.yaml`) and update the placeholder values before applying.

<Tabs>
  <Tab title="Helm Chart">
    Point ArgoCD at a Helm repository containing your Control Plane CRD templates:

    ```yaml
    apiVersion: argoproj.io/v1alpha1
    kind: Application
    metadata:
      name: my-helm-app
      namespace: argocd  # This is usually where ArgoCD is installed
    spec:
      project: default
      destination:
        server: https://kubernetes.default.svc  # Cluster API server URL
        namespace: your-namespace  # Target namespace in your cluster
      source:
        repoURL: https://your-org.github.io/your-repo/  # URL of your Helm repository
        chart: my-cpln-chart  # Name of your Helm chart
        targetRevision: 0.1.0  # Chart version
        helm:
          values: |
            org: your-org-name
      syncPolicy:
        automated:
          prune: true  # Automatically delete resources no longer defined in the chart
          selfHeal: true  # Automatically sync drifted resources
    ```

    To create your own Helm chart:

    1. Create a new Helm chart: `helm create my-cpln-chart`
    2. Add your Control Plane CRD manifests to the `templates/` directory
    3. Push the chart to a GitHub repository
    4. [Publish the chart](https://helm.sh/docs/howto/chart_releaser_action/) to GitHub Pages or a Helm repository
  </Tab>
  <Tab title="YAML Manifests">
    Point ArgoCD at a Git repository containing Control Plane CRD manifests:

    ```yaml
    apiVersion: argoproj.io/v1alpha1
    kind: Application
    metadata:
      name: my-cpln-app
      namespace: argocd  # This is usually where ArgoCD is installed
    spec:
      project: default
      destination:
        server: https://kubernetes.default.svc  # Cluster API server URL
        namespace: your-namespace  # Target namespace in your cluster
      source:
        repoURL: https://github.com/your-org/your-repo.git  # Your Git repository URL
        path: manifests  # Path to the directory containing YAML files
        targetRevision: main  # Branch, tag, or commit
      syncPolicy:
        automated:
          prune: true  # Automatically delete resources no longer defined in the repo
          selfHeal: true  # Automatically sync drifted resources
    ```
  </Tab>
</Tabs>

Apply your ArgoCD Application:

```bash
kubectl -n argocd apply -f app.yaml
```

### Example application

The [k8s-operator repository](https://github.com/controlplane-com/k8s-operator) includes a ready-to-use example that deploys a GVC, workload, identity, and other resources.

Copy the following manifest and save it to a file (e.g., `example-app.yaml`):

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: my-helm-app
  namespace: argocd
spec:
  project: default
  destination:
    server: 'https://kubernetes.default.svc'
    namespace: fresh
  source:
    repoURL: 'https://cuppojoe.github.io/argo-example/'
    chart: argo-example
    targetRevision: 0.2.3
    helm:
      values: |
        org: YOUR_ORG_NAME  # Replace with your Control Plane org name
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
```

**Before applying:**
- Replace `YOUR_ORG_NAME` with your actual Control Plane org name

Apply the manifest:

```bash
kubectl apply -f example-app.yaml
```

The example Helm chart creates the following Control Plane resources:
- **GVC** named `fresh` in `aws-eu-central-1`
- **Workload** with a serverless container
- **Identity** for cloud access
- **Policy** for permissions
- **Secret** for credentials
- Additional resources (agent, domain, ipset)

### Connecting to the ArgoCD UI

To access the ArgoCD UI, retrieve the admin password and port-forward the service:

```bash
# Print the initial admin password
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath='{.data.password}' | base64 -d && echo

# Port-forward to the ArgoCD UI
kubectl -n argocd port-forward service/argocd-server 18081:443
```

Open a browser and navigate to `https://localhost:18081`. Accept the self-signed certificate and log in with username `admin` and the password from the command above.

<Tip>
Store your Control Plane CRD manifests in a Git repository and point ArgoCD at it. Changes merged to your main branch will automatically sync to Control Plane.
</Tip>

## Secrets handling

For security reasons, secret data must be stored using a native Kubernetes Secret object, not a custom resource. This makes secrets slightly different from other resource types.

<Note>
The `app.kubernetes.io/managed-by: cpln-operator` label is required for the operator to manage the secret. Secrets without this label are ignored.
</Note>

```yaml
apiVersion: v1
kind: Secret
type: opaque
metadata:
  name: my-secret
  namespace: default
  labels:
    app.kubernetes.io/managed-by: cpln-operator  # Required for operator to manage this secret
  annotations:
    cpln.io/org: your-org-name  # Required: your Control Plane org name
data:
  encoding: cGxhaW4=  # base64 encoded: plain
  payload: c2VjcmV0LXZhbHVl  # base64 encoded: secret-value
```

## Uninstalling

### Remove operator credentials

Remove the authentication secret:

```bash
cpln operator uninstall --org YOUR_ORG_NAME
```

Or manually:

```bash
kubectl delete secret YOUR_ORG_NAME -n controlplane
```

### Remove the operator

```bash
helm uninstall cpln-operator -n controlplane
```

### Remove cert-manager (optional)

If you no longer need cert-manager:

```bash
kubectl delete -f https://github.com/cert-manager/cert-manager/releases/download/v1.16.3/cert-manager.yaml
```

<Warning>
Uninstalling the operator does not delete Control Plane resources that were created by it. Resources in Control Plane will continue to exist.
</Warning>

## Troubleshooting

<AccordionGroup>
<Accordion title="Operator pods not starting">
  Check cert-manager is running:

  ```bash
  kubectl get pods -n cert-manager
  ```

  Verify webhook certificates:

  ```bash
  kubectl get certificates -n controlplane
  ```
</Accordion>

<Accordion title="Resources not syncing">
  Check operator logs:

  ```bash
  kubectl logs -n controlplane -l app=operator -f
  ```

  Verify the authentication secret exists (look for a secret named after your org):

  ```bash
  kubectl get secrets -n controlplane
  ```

  If the secret for your org doesn't exist, run the [`cpln operator install`](#using-the-cli-recommended) command to create it.
</Accordion>

<Accordion title="Permission denied errors">
  Ensure the service account has appropriate permissions in Control Plane. Check which group and/or policy the service account belongs to and verify it has the necessary permissions.

  To reconfigure authentication, run the [`cpln operator install`](#using-the-cli-recommended) command which will create a service account and add it to the `superusers` group.
</Accordion>

<Accordion title="CRD validation errors">
  View the full resource spec available for each CRD:

  ```bash
  kubectl explain gvc.spec
  kubectl explain workload.spec
  ```
</Accordion>

<Accordion title="Namespace doesn't exist">
  Create the controlplane namespace:

  ```bash
  kubectl create namespace controlplane
  ```
</Accordion>
</AccordionGroup>

## Next steps

<CardGroup cols={2}>
  <Card title="GVC Reference" href="/reference/gvc" icon="globe">
    Learn about GVC configuration options
  </Card>
  <Card title="Workload Reference" href="/reference/workload/general" icon="server">
    Explore workload deployment options
  </Card>
  <Card title="Identity Reference" href="/reference/identity" icon="fingerprint">
    Configure cloud access and permissions
  </Card>
  <Card title="Policy Reference" href="/reference/policy" icon="shield">
    Configure access control rules
  </Card>
</CardGroup>
